kmeans_clust$centers
kmeans_clust = kmeans(listings_numeric,4, iter=100)
kmeans_clust$centers
kmeans_clust = kmeans(listings_numeric,5, iter=100)
kmeans_clust$centers
kmeans_clust = kmeans(listings_numeric,6, iter=100)
kmeans_clust$centers
kmeans_clust = kmeans(listings_numeric,10, iter=100)
kmeans_clust$centers
kmeans_clust = kmeans(listings_numeric,6, iter=100)
kmeans_clust$centers
table(kmeans_clust)
table(kmeans_clust$cluster)
set.seed(123)
kmeans_clust = kmeans(listings_numeric,5, iter=100)
kmeans_clust$centers
kmeans_clust = kmeans(listings_numeric,6, iter=100)
kmeans_clust$centers
table(kmeans_clust$cluster)
table(kmeans_clust$cluster)
set.seed(100)
kmeans_clust = kmeans(listings_numeric,5, iter=100)
kmeans_clust$centers
table(kmeans_clust$cluster)
kmeans_clust = kmeans(listings_numeric,10, iter=100)
kmeans_clust$centers
table(kmeans_clust$cluster)
kmeans_clust = kmeans(listings_numeric,5, iter=100)
kmeans_clust$centers
table(kmeans_clust$cluster)
set.seed(100)
kmeans_clust = kmeans(listings_numeric,5, iter=100)
kmeans_clust$centers
table(kmeans_clust$cluster)
hist(listings$price)
set.seed(100)
kmeans_clust = kmeans(listings_numeric,5, iter=100)
kmeans_clust$centers
table(kmeans_clust$cluster)
set.seed(100)
kmeans_clust = kmeans(listings_numeric,5, iter=1000)
kmeans_clust$centers
table(kmeans_clust$cluster)
commentsTrain
kmeans_clust$cluster
kmeans_clust$centers
table(kmeans_clust$cluster)
table(commentsTrain$avg_rating)
commentsTest$avg_rating .== "High"
commentsTest$avg_rating .== rep("High")
commentsTest$avg_rating[1:10] .== rep("High",10)
rep("High",10)
commentsTest$avg_rating[1:10] .== rep("High",10)
commentsTest$avg_rating[1:10] == rep("High",10)
commentsTest$avg_rating == rep("High",nrow(commentsTest))
table(commentsTest)
table(commentsTest$avg_rating)
table(commentsTest$avg_rating)["High"]
table(commentsTest$avg_rating)["High"]/nrow(commentsTest)
sum(diag(confusionMatrix))/nrow(commentsTest)
library(ggmap)
# requires internet connection
bos_plot=ggmap(get_map('Boston, Massachusetts',zoom=13,source='google',maptype='terrain'))
listings_plot = listings %>% select(latitude, longitude)
bos_plot +
geom_point(data=listings_plot,aes(x=listings_plot$longitude,y=listings_plot$latitude),
col='blue', alpha=.5,size=1)
str(listings_plot)
kmeans_clust$centers
kmeans_clust$cluster
length(kmeans_clust$cluster)
length(listings_numeric)
dim(listings_numeric)
str(listings_numeric)
library(RColorBrewer)
help(kmeans)
kmeans_clust = kmeans(listings_numeric,5, iter=1000, nstart=10)
length(kmeans_clust$cluster)
# Look at the average values of the clusters
kmeans_clust$centers
table(kmeans_clust$cluster)
set.seed(100)
kmeans_clust = kmeans(listings_numeric,5, iter=1000, nstart=10)
length(kmeans_clust$cluster)
# Look at the average values of the clusters
kmeans_clust$centers
table(kmeans_clust$cluster)
kmeans_clust = kmeans(listings_numeric,5, iter=1000, nstart=100)
length(kmeans_clust$cluster)
# Look at the average values of the clusters
kmeans_clust$centers
table(kmeans_clust$cluster)
set.seed(100)
kmeans_clust = kmeans(listings_numeric,5, iter=1000, nstart=100)
length(kmeans_clust$cluster)
# Look at the average values of the clusters
kmeans_clust$centers
set.seed(123)
kmeans_clust = kmeans(listings_numeric,5, iter=1000, nstart=100)
length(kmeans_clust$cluster)
# Look at the average values of the clusters
kmeans_clust$centers
set.seed(1234)
kmeans_clust = kmeans(listings_numeric,5, iter=1000, nstart=100)
length(kmeans_clust$cluster)
# Look at the average values of the clusters
kmeans_clust$centers
set.seed(1234)
kmeans_clust = kmeans(listings_numeric,5, iter=1000, nstart=100)
length(kmeans_clust$cluster)
# Look at the average values of the clusters
kmeans_clust$centers
table(kmeans_clust$cluster)
# What are the characteristics of these 5 clusters?
listings_numeric = listings %>% select(id,accommodates, bathrooms, bedrooms,
review_scores_rating, price) %>%
na.omit()
listings_numeric[,!id]
listings_numeric[,-1]
listings_numeric = listings %>% select(id,accommodates, bathrooms, bedrooms,
review_scores_rating, price) %>%
na.omit()
# K-means clustering
set.seed(1234)
kmeans_clust = kmeans(listings_numeric[,-1],5, iter=1000, nstart=100)
length(kmeans_clust$cluster)
listings_numeric = listings %>% select(id,latitude,longitude,
accommodates, bathrooms,
bedrooms, review_scores_rating,
price) %>%
na.omit()
set.seed(1234)
kmeans_clust = kmeans(listings_numeric[,-1:-3],5, iter=1000, nstart=100)
length(kmeans_clust$cluster)
# Look at the average values of the clusters
kmeans_clust$centers
table(kmeans_clust$cluster)
library(ggmap)
library(RColorBrewer)
# requires internet connection
bos_plot=ggmap(get_map('Boston, Massachusetts',zoom=13,source='google',maptype='terrain'))
listings_plot = listings_numeric %>% select(latitude, longitude)
bos_plot +
geom_point(data=listings_plot,aes(x=listings_plot$longitude,y=listings_plot$latitude),
col=listings_plot$longitude, alpha=.5,size=1)
listings_plot = listings_numeric %>% select(latitude, longitude)
bos_plot +
geom_point(data=listings_plot,aes(x=longitude,y=latitude),
col='blue', alpha=.5,size=1)
listings_plot = listings_numeric %>% select(latitude, longitude)
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=kmeans_clust$cluster, alpha=.5,size=1)
library(RColorBrewer)
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=kmeans_clust$cluster, alpha=.5,size=1) +
theme(legend.position = "bottom")
c(1:5)
display.brewer.all()
colors = brewer.pal(8, "Set1")
colors = brewer.pal(5, "Set1")
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=kmeans_clust$cluster, alpha=.5,size=1) +
scale_color_manual(name = "Year", # or name = element_blank()
labels = c(1:5),
values = colors)
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=kmeans_clust$cluster, alpha=.5,size=1) +
scale_color_manual(name = "Year", # or name = element_blank()
labels = c(1:5),
values = colors)
colors
kmeans_clust$cluster
kmeans_clust$cluster[1]
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=kmeans_clust$cluster, alpha=.5,size=1) +
scale_color_manual(name = "Year", # or name = element_blank()
labels = c("1","2","3","4","5"),
values = colors)
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=kmeans_clust$cluster, alpha=.5,size=1)
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=factor(kmeans_clust$cluster), alpha=.5,size=1)
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=factor(kmeans_clust$cluster), alpha=.5,size=1) +
scale_color_manual(name = "Year",
labels = c("1","2","3","4","5"),
values = colors)
display.brewer.all()
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude),
col=kmeans_clust$cluster, alpha=.5,size=1) +
scale_colour_brewer(palette = "Set1")
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude,colour=kmeans_clust$cluster),
alpha=.5,size=1) +
scale_colour_brewer(palette = "Set1")
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude,colour=factor(kmeans_clust$cluster)),
alpha=.5,size=1) +
scale_colour_brewer(palette = "Set1")
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude,colour=factor(kmeans_clust$cluster)),
alpha=.5,size=1) +
scale_colour_brewer("Cluster", palette = "Set1")
bos_plot +
geom_point(data=listings_numeric,aes(x=longitude,y=latitude,colour=factor(kmeans_clust$cluster)),
alpha=.5,size=1) +
xlab("Longitude") + ylab("Latitude") +
scale_colour_brewer("Cluster", palette = "Set1")
prp(commentsCART)
table(kmeans_clust$cluster)
kmeans_clust$centers
output: html_document
knitr::opts_chunk$set(echo = TRUE)
```
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
plot(pressure)
summary(cars)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/cos_2017/3_modeling_and_ml")
listings = read.csv("boston-airbnb-open-data/listings.csv")
calendar = read.csv("boston-airbnb-open-data/calendar.csv")
reviews = read.csv("boston-airbnb-open-data/reviews.csv", stringsAsFactors = FALSE)
summary(cars)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
str(reviews)
sort(table(reviews$listing_id), decreasing=T)[1:100]
sort(listings$number_of_reviews, decreasing=T)[1:100]
sort(table(reviews$reviewer_name),decreasing=T)[1:10]
names(listings_scores)
listings_scores = listings %>% select(id, number_of_reviews,
"rating"=review_scores_rating)
listings_scores
str(listings_scores)
listings_scores = listings %>% select(id, "number"=number_of_reviews,
"rating"=review_scores_rating)
filter(number > 0) %>%
filter(!is.na(rating))
listings_scores = listings %>% select(id, "num"=number_of_reviews,
"rating"=review_scores_rating)
filter(num > 0) %>%
filter(!is.na(rating))
listings_scores = listings %>% select(id, "number"=number_of_reviews,
"rating"=review_scores_rating) %>%
filter(num > 0) %>%
filter(!is.na(rating))
listings_scores = listings %>% select(id, "number"=number_of_reviews,
"rating"=review_scores_rating) %>%
filter(number > 0) %>%
filter(!is.na(rating))
str(listings_scores)
str(reviews_by_listing)
inspect(frequencies)
inspect(frequencies[1:15,])
inspect(frequencies[1:15,1:nrow(frequencies)])
inspect(frequencies[1:15,1:1000])
inspect(frequencies[1:15,1:30])
findFreqTerms(frequencies, lowfreq=10000)
strwrap(corpus[[1]])[1:100]
strwrap(corpus[[1]])[1:20]
strwrap(corpus[[1]])[1:5]
inspect(frequencies[1:15,1775:1800])
frequencies
colnames(commentsTM)
str(commentsTM, list.len=10)
sparse = removeSparseTerms(frequencies, 0.95)
# How many did we keep? (3043, 4576)
dim(sparse)
## Step 8: Create data frame
commentsTM = as.data.frame(as.matrix(sparse))
# View data frame (rows are reviews, columns are words)
dim(commentsTM)
names(commentsTM)
commentsTM = commentsTM[,!grepl("[0-9]",names(commentsTM))]
commentsTM = na.omit(commentsTM)
str(commentsTM, list.len=10)
dim(sparse)
commentsTM = as.data.frame(as.matrix(sparse))
# View data frame (rows are reviews, columns are words)
str(commentsTM, list.len=10)
# Drop columns that include numbers
commentsTM = commentsTM[,!grepl("[0-9]",names(commentsTM))]
str(commentsTM, list.len=10)
review_scores
full_join(commentsTM, listings_scores)
listings_scores
str(listings_scores)
listings_scores = listings %>%
filter(number_of_reviews > 0) %>%
select("listing_id"=id, "rating"=review_scores_rating) %>%
filter(!is.na(rating))
str(listings_scores)
# Add our dependent variable:
commentsTM$listing_id = reviews_by_listing$listing_id
str(commentsTM)
str(listings_scores)
commentsTM = full_join(commentsTM, listings_scores)
names(commentsTM)
listings_scores = listings %>%
filter(number_of_reviews > 0) %>%
select("LISTING_ID"=id, "RATING"=review_scores_rating) %>%
filter(!is.na(RATING))
str(listings_scores)
listings_scores$RATING_TEXT = listings_scores$RATING %>%
sapply(convert_rating) %>%
as.factor()
str(listings_scores)
reviews_by_listing$all_comments[1]
reviews_by_listing$all_comments[1][1:100]
reviews_by_listing$all_comments[1]
head(reviews_by_listing$all_comments[1])
reviews_by_listing$all_comments[[1]]
reviews_by_listing$all_comments[[1]][1:10]
substr(reviews_by_listing$all_comments[1],1,100)
substr(reviews_by_listing$all_comments[1],1,1000)
reviews_by_listing$all_comments[1]
library(caTools)
names(commentsTM)
commentsTM$LISTING_ID = reviews_by_listing$listing_id
commentsTM = full_join(commentsTM, listings_scores)
# Remove all rows with NA's
commentsTM = na.omit(commentsTM)
# View data frame columns
names(commentsTM)
library(caTools)
set.seed(123)
spl = sample.split(commentsTM$RATING_TEXT, SplitRatio = 0.7)
commentsTrain = subset(commentsTM, spl==TRUE)
commentsTest = subset(commentsTM, spl==FALSE)
str(commentsTrain)
names(commentsTrain)
commentsCART = rpart(RATING_TEXT ~ ., data=commentsTrain, method="class")
prp(commentsCART)
commentsCART = rpart(RATING_TEXT ~ . - rating, data=commentsTrain, method="class")
prp(commentsCART)
commentsTM = as.data.frame(as.matrix(sparse))
# View data frame (rows are reviews, columns are words)
str(commentsTM, list.len=10)
# Drop columns that include numbers
commentsTM = commentsTM[,!grepl("[0-9]",names(commentsTM))]
str(listings_scores)
# Add our dependent variable:
commentsTM$LISTING_ID = reviews_by_listing$listing_id
commentsTM = full_join(commentsTM, listings_scores)
# Remove all rows with NA's
commentsTM = na.omit(commentsTM)
# View data frame columns
names(commentsTM)
# commentsTM$avg_rating = sapply(reviews_by_listing$listing_id,
#                                function(id) get_rating=avg_rating[toString(id)])
# Now we are ready to build models.
# Note: Up to here, we have just pre-processed and prepared our data.
# Use the Bag-of-words to build a CART model to predict review scores
# Split data into training and testing sets
# install.packages("caTools")
library(caTools)
set.seed(123)
spl = sample.split(commentsTM$RATING_TEXT, SplitRatio = 0.7)
commentsTrain = subset(commentsTM, spl==TRUE)
commentsTest = subset(commentsTM, spl==FALSE)
# Let's use CART! Why CART?
# install.packages("rpart")
library(rpart)
# install.packages("rpart.plot")
library(rpart.plot)
# Train the model
# (of course, we cannot include RATING or LISTING_ID as predictive variables)
commentsCART = rpart(RATING_TEXT ~ . - RATING - LISTING_ID, data=commentsTrain, method="class")
prp(commentsCART)
PredictCART = predict(commentsCART, newdata=commentsTest, type="class")
confusionMatrix = table(commentsTest$RATING_TEXT, PredictCART)
confusionMatrix
# Accuracy?
sum(diag(confusionMatrix))/nrow(commentsTest)
# Most frequent response variable in training set is "High"
# => Baseline accuracy is 0.2720
table(commentsTest$RATING_TEXT)["High"]/nrow(commentsTest)
names(commentsTM)[1:10]
length(commentsTM)
num_cols = length(commentsTM)
names(commentsTM)[c(-10)]
num_cols
names(commentsTM)[(length(commentsTM)-10):]
names(commentsTM)[c((length(commentsTM)-10):)]
names(commentsTM)[c(length(commentsTM)-10:)]
names(commentsTM)[length(commentsTM)-10:]
names(commentsTM)[length(commentsTM)-10]
names(commentsTM)[(length(commentsTM)-10):length(commentsTM)]
N
names(commentsTM)[1:5,N-5:N]
N = length(commentsTM)
names(commentsTM)[1:5,N-5:N]
names(commentsTM)[c(1:5,N-5:N)]
names(commentsTM)[c(1:5,(N-5):N)]
names(commentsTM)[c((N-10):N)]
head(reviews,3)
str(listings, list.len=10)
str(listings, list.len=5)
display.brewer.all()
# install.packages("RColorbrewer")
library(RColorBrewer)
display.brewer.all()
colors = brewer.pal(5, "Set1")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: intro
#### Intro stuff to be loaded in during part 1 ####
setwd("~/cos_2017/3_modeling_and_ml")
# install.packages("dplyr")
library(dplyr)
# install.packages("rpart")
library(rpart)
# install.packages("tidyverse")
library(tidyverse) # lots of conflicts with other packages
#
# listings = read.csv("boston-airbnb-open-data/listings.csv")
# calendar = read.csv("boston-airbnb-open-data/calendar.csv")
# Chunk 3: view_reviews
reviews = read.csv("boston-airbnb-open-data/reviews.csv", stringsAsFactors = FALSE)
head(reviews, 3)
# Chunk 4: reviewer_names
sort(table(reviews$reviewer_name),decreasing = TRUE)[1:10]
# Chunk 5: review_scores
listings = read.csv("boston-airbnb-open-data/listings.csv")
listings_scores = listings %>%
filter(number_of_reviews > 0) %>%
select("LISTING_ID"=id, "RATING"=review_scores_rating) %>%
filter(!is.na(RATING))
str(listings_scores)
# Chunk 6: convert_rating
convert_rating <- function(rating){
if(rating == 100){
return("Perfect")
}else if(rating >= 95){
return("High")
}else if(rating >= 90){
return("Mid")
}else if(rating >= 80){
return("Low")
}else{
return("Terrible")
}
}
# Chunk 7: exercise dplyr_convert_rating
listings_scores$RATING_TEXT = listings_scores$RATING %>%
sapply(convert_rating) %>%
as.factor()
# Chunk 8: view_RATING_TEXT
table(listings_scores$RATING_TEXT)
# Chunk 9: reviews_by_listing
reviews_by_listing = reviews %>%
select(listing_id,comments) %>%
group_by(listing_id) %>%
summarise(all_comments=paste(comments,collapse=" "))
# Check out the updated data frame - 2829 rows
str(reviews_by_listing)
# Chunk 10: text_pre_processing
# install.packages("tm")
library(tm)
# install.packages("SnowballC")
library(SnowballC)
# Chunk 11: corpus_1
corpus = Corpus(VectorSource(reviews_by_listing$all_comments))
# Take a look
strwrap(corpus[[1]])[1:3]
# Chunk 12: corpus_2
corpus = tm_map(corpus, tolower)
# IMPORTANT STEP BEFORE CONTINUING (required after to_lower function)
corpus = tm_map(corpus, PlainTextDocument)
# Take a look
strwrap(corpus[[1]])[1:3]
# Chunk 13: corpus_3
corpus = tm_map(corpus, removePunctuation)
# Take a look
strwrap(corpus[[1]])[1:3]
# Chunk 14: corpus_4
# Take a look at tm's stopwords:
stopwords("english")[1:10]
# Just remove stopwords: (this step may take a minute)
corpus = tm_map(corpus, removeWords, stopwords("english"))
display.brewer.all(5)
pdf("display_all")
display.brewer.all()
dev.off()
pdf("display_all", width=10, height=8)
display.brewer.all()
dev.off()
pdf("display_all", width=10, height=4)
display.brewer.all()
dev.off()
pdf("display_all", width=10, height=8)
display.brewer.all()
dev.off()
version # outputs 3.3.2
library(tm)
version # version.string >= 3.3.2
library(tm)
