{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning & Neural Networks\n",
    "## Project 2 - Binary Encodings\n",
    "\n",
    "In this project we will implement an autoencoder of the numbers 1-8 where the numbers are given as 1-hot encodings. e.g. $(1,0,0,0,0,0,0,0)$ for $1$ and $(0,1,0,0,0,0,0,0)$ for $2$, etc.\n",
    "\n",
    "We will try to achieve a 3-dimensional represenation of this data. Clearly, we know this should be binary $(0, 0, 0)$ for $1$, $(0, 0, 1)$ for 2, etc. However, we will see if the neural network can work this out for itself; we expect it (almost) will except for perhaps assinging a different order to the binary numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will load all the necessary libraries, which are just TensorFlow, Numpy and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load TensorFlow\n",
    "import tensorflow as tf\n",
    "# Load numpy - adds MATLAB/Julia-style math to Python\n",
    "import numpy as np\n",
    "# Load matplotlib for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is just an $8 \\times 8$ identity matrix. Using matplotlib we can also plot a heatmap of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the one-hot encodings\n",
    "# Symmetric matrix, so doesn't really matter\n",
    "# But for sanity, we'll think of row = number\n",
    "data = np.eye(8).astype('float32')\n",
    "# Plot it\n",
    "plt.matshow(data, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the TensorFlow graph for our autoencoder\n",
    "Let's create our computation graph\n",
    "#### The encoder\n",
    "\n",
    "Our encoder is a fully-conntected layer followed with a sigmoid, with no bias term. So mathematically we can write it as \n",
    "\n",
    "$$y = \\sigma(W x)$$\n",
    "\n",
    "where $y$ is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 8\n",
    "CODE_DIM = 3\n",
    "\n",
    "enc_weight = tf.Variable(tf.random_uniform([INPUT_DIM,CODE_DIM], -1.0, +1.0))\n",
    "enc_input  = tf.matmul(data, enc_weight)\n",
    "enc_output = tf.nn.sigmoid(enc_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The decoder\n",
    "\n",
    "This is a fully-connected layer followed by softmax, again with no bias.\n",
    "\n",
    "We'll be fancy here: let's use the same weights, just transposed! So mathematically it's\n",
    "\n",
    "$$x' = softmax(W^\\top y)$$\n",
    "\n",
    "where $x'$ is the reconstructed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dec_weight = tf.transpose(enc_weight) #tf.Variable(tf.random_uniform([CODE_DIM,INPUT_DIM], -1.0, +1.0))\n",
    "dec_weight = tf.Variable(tf.random_uniform([CODE_DIM,INPUT_DIM], -1.0, +1.0))\n",
    "dec_input  = tf.matmul(enc_output,dec_weight)\n",
    "dec_output = tf.nn.softmax(dec_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can do this, let's summarize everything with math. Our autoencoder is basically the function\n",
    "\n",
    "$$x' = softmax(W^\\top \\sigma(W x)) $$\n",
    "\n",
    "and to train the autoencoder we need to solve the non-linear, non-convex optimization problem\n",
    "\n",
    "$$\\min_{W \\in \\mathbb{R}^{8 \\times 3}} \\left\\lVert softmax(W^\\top \\sigma(W x)) - x \\right\\rVert_2^2$$\n",
    "\n",
    "and we will do this with Gradient Descent, as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training operation\n",
    "\n",
    "The error we want to minimize is given with the following tensorflow code that implements that sum of squared differences between the data and the decoder's output (this corresponds to the objective function of the above optimization problem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = tf.reduce_sum(tf.square(data - dec_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the previous exercise, we will declare a gradient descent optimizer and run the initialization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "train = optimizer.minimize(error)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of output do we get right now with the network initialized to random weights? Let's plot this using matplotlib again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(sess.run(dec_output), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly it's not what we want.\n",
    "\n",
    "Let's now train our autoencoder so it does roughly what it's supposed to! Again we call the train operation (a gradient descent step) many times from our ``session`` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run some gradient steps\n",
    "errors = []\n",
    "N_STEPS = 5000\n",
    "for step in range(N_STEPS):\n",
    "    cur_error, _ = sess.run((error,train))\n",
    "    errors.append(cur_error)\n",
    "    if step % 100 == 0:\n",
    "        print step, cur_error\n",
    "plt.plot(range(N_STEPS), errors, 'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our autoencoder reproduce the input now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(sess.run(dec_output), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost perfectly, and what does the coding look like? We can plot the intermediate layer of our network by running it via the ``Session`` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(sess.run(enc_output), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can round the codings and then print all the unique binary numbers... hopefully there are close to 8 of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(map(lambda l: int(''.join(map(str, l)), 2), np.round(sess.run(enc_output)).astype(int)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
